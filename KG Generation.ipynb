{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download de_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('de_dep_news_trf')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "#pd.set_option('display.max_colwidth', 200)\n",
    "#%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"KE1.txt\", \"r\") as f:\n",
    "    txt = f.read()\n",
    "\n",
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter sb PROPN \n",
      "mag ROOT VERB Peter\n",
      "Ute oa PROPN \n",
      ". punct PUNCT \n"
     ]
    }
   ],
   "source": [
    "for tok in nlp(\"Peter mag Ute.\"):\n",
    "    print(tok, tok.dep_, tok.pos_, list(tok.children)[0] if len(list(tok.children)) > 0 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"ROOT\", \"ac\", \"adc\", \"ag\", \"ams\", \"app\", \"avc\", \"cc\", \"cd\", \"cj\", \"cm\", \"cp\", \"cvc\", \"da\", \"dep\", \"dm\", \"ep\", \"ju\", \"mnr\", \"mo\", \"ng\", \"nk\", \"nmc\", \"oa\", \"oc\", \"og\", \"op\", \"par\", \"pd\", \"pg\", \"ph\", \"pm\", \"pnc\", \"punct\", \"rc\", \"re\", \"rs\", \"sb\", \"sbp\", \"svp\", \"uc\", \"vo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT None\n",
      "ac adpositional case marker\n",
      "adc adjective component\n",
      "ag genitive attribute\n",
      "ams measure argument of adjective\n",
      "app apposition\n",
      "avc adverbial phrase component\n",
      "cc coordinating conjunction\n",
      "cd coordinating conjunction\n",
      "cj conjunct\n",
      "cm comparative conjunction\n",
      "cp complementizer\n",
      "cvc collocational verb construction\n",
      "da dative\n",
      "dep unclassified dependent\n",
      "dm discourse marker\n",
      "ep expletive es\n",
      "ju junctor\n",
      "mnr postnominal modifier\n",
      "mo modifier\n",
      "ng negation\n",
      "nk noun kernel element\n",
      "nmc numerical component\n",
      "oa accusative object\n",
      "oc clausal object\n",
      "og genitive object\n",
      "op prepositional object\n",
      "par parenthetical element\n",
      "pd predicate\n",
      "pg phrasal genitive\n",
      "ph placeholder\n",
      "pm morphological particle\n",
      "pnc proper noun component\n",
      "punct punctuation\n",
      "rc relative clause\n",
      "re repeated element\n",
      "rs reported speech\n",
      "sb subject\n",
      "sbp passivized subject (PP)\n",
      "svp separable verb prefix\n",
      "uc unit component\n",
      "vo vocative\n"
     ]
    }
   ],
   "source": [
    "for i in l:\n",
    "    print(i, spacy.explain(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sentence segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Entity extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    #print(tok.dep_)\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      '''# check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text'''\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_ == \"sb\":#if tok.dep_.find(\"subj\") == True:\n",
    "        #print(tok, tok.dep_)\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_ == \"oa\":##if tok.dep_.find(\"obj\") == True:\n",
    "        #print(tok, tok.dep_)\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1129it [00:40, 27.71it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_pairs = []\n",
    "\n",
    "for i in tqdm(doc.sents):\n",
    "  entity_pairs.append(get_entities(i.text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Relations Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "\n",
    "  doc = nlp(sent)\n",
    "\n",
    "  # Matcher class object \n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  #define the pattern \n",
    "  pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "  matcher.add(\"matching_1\", [pattern]) \n",
    "\n",
    "  matches = matcher(doc)\n",
    "  k = len(matches) - 1\n",
    "\n",
    "  if k == -1:\n",
    "    return\n",
    "\n",
    "  span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "  return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1129it [00:42, 26.87it/s]\n"
     ]
    }
   ],
   "source": [
    "relations = [get_relation(i.text) for i in tqdm(doc.sents)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in entity_pairs]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in entity_pairs]\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. Bitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. Klicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. Weitere Details finden Sie in Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"ist\"], \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Using cached flair-0.11.3-py3-none-any.whl (401 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (2022.10.31)\n",
      "Collecting segtok>=1.5.7\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (8.10.0)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (1.0.2)\n",
      "Collecting mpld3==0.3\n",
      "  Using cached mpld3-0.3-py3-none-any.whl\n",
      "Collecting wikipedia-api\n",
      "  Using cached Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Using cached konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (4.62.3)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Using cached Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Using cached sqlitedict-2.1.0-py3-none-any.whl\n",
      "Collecting janome\n",
      "  Using cached Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Collecting gdown==4.4.0\n",
      "  Using cached gdown-4.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: lxml in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (4.9.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (0.4.0)\n",
      "Requirement already satisfied: transformers>=4.0.0 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (4.15.0)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (1.13.1)\n",
      "Collecting gensim>=3.4.0\n",
      "  Using cached gensim-4.3.0-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (0.1.95)\n",
      "Requirement already satisfied: tabulate in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (3.4.3)\n",
      "Collecting conllu>=4.0\n",
      "  Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from flair) (2.8.2)\n",
      "Collecting pptree\n",
      "  Using cached pptree-3.1-py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (4.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\menze\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\menze\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (2.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\menze\\anaconda3\\lib\\site-packages (from gdown==4.4.0->flair) (3.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\menze\\anaconda3\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.20.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (2.0.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: Cython==0.29.32 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (0.29.32)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\menze\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim>=3.4.0->flair) (1.3.4)\n",
      "Requirement already satisfied: pyfume in c:\\users\\menze\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim>=3.4.0->flair) (0.2.25)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.6.3)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\menze\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (2.0.0)\n",
      "Requirement already satisfied: future in c:\\users\\menze\\anaconda3\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Collecting py4j\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Using cached overrides-3.1.0-py3-none-any.whl\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Using cached importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.6.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (2.2.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\menze\\anaconda3\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\menze\\anaconda3\\lib\\site-packages (from tqdm>=4.26.0->flair) (0.4.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (6.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\menze\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (0.0.47)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (0.10.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from transformers>=4.0.0->flair) (21.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.2.1)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim>=3.4.0->flair) (2021.3)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\menze\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=3.4.0->flair) (1.8.1)\n",
      "Requirement already satisfied: simpful in c:\\users\\menze\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim>=3.4.0->flair) (2.9.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\menze\\anaconda3\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim>=3.4.0->flair) (0.0.6)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\menze\\anaconda3\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: click in c:\\users\\menze\\anaconda3\\lib\\site-packages (from sacremoses->transformers>=4.0.0->flair) (8.0.3)\n",
      "Installing collected packages: py4j, overrides, importlib-metadata, gensim, wikipedia-api, sqlitedict, segtok, pptree, mpld3, langdetect, konoha, janome, hyperopt, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.8.1\n",
      "    Uninstalling importlib-metadata-4.8.1:\n",
      "      Successfully uninstalled importlib-metadata-4.8.1\n",
      "Successfully installed bpemb-0.3.4 conllu-4.5.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 gdown-4.4.0 gensim-4.3.0 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 pptree-3.1 py4j-0.10.9.7 segtok-1.5.11 sqlitedict-2.1.0 wikipedia-api-0.5.8\n"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-08 11:30:08,529 loading file C:\\Users\\menze\\.flair\\models\\de-pos-ud-hdt-v0.5.pt\n",
      "2023-02-08 11:30:08,700 SequenceTagger predicts: Dictionary with 58 tags: <unk>, O, APPR, ART, ADJA, NN, VVFIN, PIS, NE, FM, $,, KON, $., CARD, APPRART, $(, PROAV, KOUS, PPER, ADV, VVINF, VAFIN, VMFIN, ADJD, PTKVZ, PTKNEG, KOKOM, PIDAT, PIAT, VVPP, PRF, PTKA, TRUNC, PPOSAT, VVIZU, PTKZU, VAINF, VMINF, PWAV, PDAT, PRELS, KOUI, APPO, VAPP, PWAT, PWS, VVIMP, APZR, PDS, PRELAT\n",
      "Sentence: \"George wurde in Washington geboren\" → [\"George\"/NE, \"wurde\"/VAFIN, \"in\"/APPR, \"Washington\"/NE, \"geboren\"/VVPP]\n",
      "2023-02-08 11:30:09,168 loading file C:\\Users\\menze\\.flair\\models\\relations-v11.pt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import RelationExtractor, SequenceTagger\n",
    "\n",
    "# 1. make example sentence\n",
    "sentence = Sentence(\"George wurde in Washington geboren\")\n",
    "\n",
    "# 2. load entity tagger and predict entities\n",
    "tagger = SequenceTagger.load('de-pos')\n",
    "tagger.predict(sentence)\n",
    "print(sentence)\n",
    "# check which entities have been found in the sentence\n",
    "entities = sentence.get_labels('ner')\n",
    "for entity in entities:\n",
    "    print(entity)\n",
    "\n",
    "# 3. load relation extractor\n",
    "extractor: RelationExtractor = RelationExtractor.load('relations')\n",
    "\n",
    "# predict relations\n",
    "extractor.predict(sentence)\n",
    "\n",
    "# check which relations have been found\n",
    "relations = sentence.get_labels('relation')\n",
    "for relation in relations:\n",
    "    print(relation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6180f3cd94dc18ccb376ae2a449b5ce04c842b9ec5e301cfdeeb81742684342e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
